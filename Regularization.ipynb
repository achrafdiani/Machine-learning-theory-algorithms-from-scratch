{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dx2p1GkwuOvt"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/dataset.csv')\n",
        "\n",
        "# Extract features (X) and labels (y)\n",
        "X = data[['x1', 'x2']].values\n",
        "y = data['Label'].values"
      ],
      "metadata": {
        "id": "SgWdneHfweB7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split_manual(X, y, train_size=0.8):\n",
        "    \"\"\"\n",
        "    Split the dataset into training and testing sets manually.\n",
        "\n",
        "    Parameters:\n",
        "    X (ndarray): The input feature data.\n",
        "    y (ndarray): The target labels.\n",
        "    train_size (float): The proportion of the dataset to include in the train split.\n",
        "\n",
        "    Returns:\n",
        "    X_train, X_test, y_train, y_test (tuple): The split data into training and testing sets.\n",
        "    \"\"\"\n",
        "    # Shuffle the dataset\n",
        "    indices = np.arange(X.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X = X[indices]\n",
        "    y = y[indices]\n",
        "\n",
        "    # Split the dataset\n",
        "    train_index = int(train_size * X.shape[0])\n",
        "    X_train, X_test = X[:train_index], X[train_index:]\n",
        "    y_train, y_test = y[:train_index], y[train_index:]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "ZzJ4VsXPwPNb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually split the data into training and testing sets\n",
        "X_train_manual, X_test_manual, y_train_manual, y_test_manual = train_test_split_manual(X, y)"
      ],
      "metadata": {
        "id": "kDBtdzKCwUiv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regide Regularization"
      ],
      "metadata": {
        "id": "2uwjM5Fbx8vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def regularized_logistic_regression_cost_function(y, h, w, lambda_):\n",
        "    \"\"\"\n",
        "    Regularized logistic regression cost function.\n",
        "\n",
        "    Parameters:\n",
        "    y (ndarray): The true labels.\n",
        "    h (ndarray): The hypothesis function values (sigmoid activation outputs).\n",
        "    w (ndarray): The weight vector of the model.\n",
        "    lambda_ (float): The regularization parameter.\n",
        "\n",
        "    Returns:\n",
        "    cost (float): The regularized cost.\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    regularization_term = (lambda_ / (2 * m)) * np.sum(w[1:] ** 2)  # Exclude the bias term from regularization\n",
        "    cost = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) + regularization_term\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "MKveVs5Gvydh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid_activation(x, w):\n",
        "\n",
        "    z = np.dot(x, w)\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "vXtvvxhfv1kr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regularized_gradient_of_cost_function(X, y, h, w, lambda_):\n",
        "    \"\"\"\n",
        "    Gradient of the regularized logistic regression cost function.\n",
        "\n",
        "    Parameters:\n",
        "    X (ndarray): The input features.\n",
        "    y (ndarray): The true labels.\n",
        "    h (ndarray): The hypothesis function values (sigmoid activation outputs).\n",
        "    w (ndarray): The weight vector of the model.\n",
        "    lambda_ (float): The regularization parameter.\n",
        "\n",
        "    Returns:\n",
        "    gradient (ndarray): The gradient of the regularized cost function.\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    regularization_term = (lambda_ / m) * w\n",
        "    regularization_term[0] = 0  # Exclude the bias term from regularization\n",
        "    gradient = (1 / m) * np.dot(X.T, (h - y)) + regularization_term\n",
        "    return gradient\n"
      ],
      "metadata": {
        "id": "r_5ifToav3kc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_regularized_logistic_regression(X, y, learning_rate=0.01, iterations=1000, lambda_=0.1):\n",
        "    \"\"\"\n",
        "    Train a regularized logistic regression model using gradient descent.\n",
        "\n",
        "    Parameters:\n",
        "    X (ndarray): The input feature data with an added bias column.\n",
        "    y (ndarray): The target labels.\n",
        "    learning_rate (float): The learning rate for gradient descent.\n",
        "    iterations (int): The number of iterations to perform gradient descent.\n",
        "    lambda_ (float): The regularization parameter.\n",
        "\n",
        "    Returns:\n",
        "    w (ndarray): The trained weights.\n",
        "    \"\"\"\n",
        "    # Initialize weights with an additional term for the bias\n",
        "    w = np.random.rand(X.shape[1] + 1)\n",
        "\n",
        "    # Add a column of ones to X to account for the bias term (intercept)\n",
        "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "    for i in range(iterations):\n",
        "        # Calculate the hypothesis/sigmoid activations\n",
        "        h = sigmoid_activation(X, w)\n",
        "\n",
        "        # Calculate the gradient with regularization\n",
        "        gradient = regularized_gradient_of_cost_function(X, y, h, w, lambda_)\n",
        "\n",
        "        # Update the weights\n",
        "        w -= learning_rate * gradient\n",
        "\n",
        "    return w\n",
        "\n",
        "    # Logistic regression prediction function\n",
        "def predict_logistic_regression(X, w):\n",
        "    \"\"\"\n",
        "    Make predictions using a logistic regression model.\n",
        "\n",
        "    Parameters:\n",
        "    X (ndarray): The input feature data.\n",
        "    w (ndarray): The trained weights.\n",
        "\n",
        "    Returns:\n",
        "    y_pred (ndarray): The predicted labels.\n",
        "    \"\"\"\n",
        "    # Add a column of ones to X to account for the bias term (intercept)\n",
        "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "    # Calculate the sigmoid activations\n",
        "    h = sigmoid_activation(X, w)\n",
        "\n",
        "    # Convert probabilities to binary predictions\n",
        "    y_pred = np.round(h)\n",
        "\n",
        "    return y_pred\n"
      ],
      "metadata": {
        "id": "lDEPT3X3wH8A"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the logistic regression model using our function\n",
        "w_trained = train_regularized_logistic_regression(X, y, learning_rate=0.01, iterations=1000, lambda_=0.1)\n",
        "\n",
        "# Make predictions on the training and testing sets\n",
        "y_train_pred_manual = predict_logistic_regression(X_train_manual, w_trained)\n",
        "y_test_pred_manual = predict_logistic_regression(X_test_manual, w_trained)\n",
        "\n",
        "# Calculate the errors manually\n",
        "train_error_manual = 1 - np.mean(y_train_pred_manual == y_train_manual)\n",
        "test_error_manual = 1 - np.mean(y_test_pred_manual == y_test_manual)\n",
        "\n",
        "train_error_manual, test_error_manual"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJpYs5KRw-wZ",
        "outputId": "b0611599-eb68-4ff0-feff-51119eb379bc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5106382978723405, 0.5416666666666667)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso regulariztion"
      ],
      "metadata": {
        "id": "4L1aUa_uyBvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lasso_logistic_regression_cost_function(y, h, w, lambda_):\n",
        "    \"\"\"\n",
        "    Lasso regularized logistic regression cost function.\n",
        "\n",
        "    Parameters:\n",
        "    y (ndarray): The true labels.\n",
        "    h (ndarray): The hypothesis function values (sigmoid activation outputs).\n",
        "    w (ndarray): The weight vector of the model.\n",
        "    lambda_ (float): The regularization parameter.\n",
        "\n",
        "    Returns:\n",
        "    cost (float): The regularized cost.\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    regularization_term = (lambda_ / m) * np.sum(np.abs(w[1:]))  # Exclude the bias term from regularization\n",
        "    cost = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) + regularization_term\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "vvPYQNU2yEqc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lasso_gradient_of_cost_function(X, y, h, w, lambda_):\n",
        "    \"\"\"\n",
        "    Gradient of the Lasso regularized logistic regression cost function.\n",
        "\n",
        "    Parameters:\n",
        "    X (ndarray): The input features.\n",
        "    y (ndarray): The true labels.\n",
        "    h (ndarray): The hypothesis function values (sigmoid activation outputs).\n",
        "    w (ndarray): The weight vector of the model.\n",
        "    lambda_ (float): The regularization parameter.\n",
        "\n",
        "    Returns:\n",
        "    gradient (ndarray): The gradient of the regularized cost function.\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    regularization_term = np.sign(w) * (lambda_ / m)\n",
        "    regularization_term[0] = 0  # Exclude the bias term from regularization\n",
        "    gradient = (1 / m) * np.dot(X.T, (h - y)) + regularization_term\n",
        "    return gradient\n"
      ],
      "metadata": {
        "id": "mtyTdo6syUiV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lasso_logistic_regression(X, y, learning_rate=0.01, iterations=1000, lambda_=0.1):\n",
        "    \"\"\"\n",
        "    Train a Lasso regularized logistic regression model using gradient descent.\n",
        "\n",
        "    Parameters:\n",
        "    X (ndarray): The input feature data with an added bias column.\n",
        "    y (ndarray): The target labels.\n",
        "    learning_rate (float): The learning rate for gradient descent.\n",
        "    iterations (int): The number of iterations to perform gradient descent.\n",
        "    lambda_ (float): The regularization parameter.\n",
        "\n",
        "    Returns:\n",
        "    w (ndarray): The trained weights.\n",
        "    \"\"\"\n",
        "    # Initialize weights with an additional term for the bias\n",
        "    w = np.random.rand(X.shape[1] + 1)\n",
        "\n",
        "    # Add a column of ones to X to account for the bias term (intercept)\n",
        "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "    for i in range(iterations):\n",
        "        # Calculate the hypothesis/sigmoid activations\n",
        "        h = sigmoid_activation(X, w)\n",
        "\n",
        "        # Calculate the gradient with L1 regularization\n",
        "        gradient = lasso_gradient_of_cost_function(X, y, h, w, lambda_)\n",
        "\n",
        "        # Update the weights\n",
        "        w -= learning_rate * gradient\n",
        "\n",
        "    return w\n",
        "\n",
        "def predict_logistic_regression(X, w):\n",
        "    \"\"\"\n",
        "    Make predictions using a logistic regression model.\n",
        "\n",
        "    Parameters:\n",
        "    X (ndarray): The input feature data.\n",
        "    w (ndarray): The trained weights.\n",
        "\n",
        "    Returns:\n",
        "    y_pred (ndarray): The predicted labels.\n",
        "    \"\"\"\n",
        "    # Add a column of ones to X to account for the bias term (intercept)\n",
        "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "    # Calculate the sigmoid activations\n",
        "    h = sigmoid_activation(X, w)\n",
        "\n",
        "    # Convert probabilities to binary predictions\n",
        "    y_pred = np.round(h)\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "BHA4425rycSD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the logistic regression model using our function\n",
        "w_trained = train_lasso_logistic_regression(X, y, learning_rate=0.01, iterations=1000, lambda_=0.1)\n",
        "\n",
        "# Make predictions on the training and testing sets\n",
        "y_train_pred_manual = predict_logistic_regression(X_train_manual, w_trained)\n",
        "y_test_pred_manual = predict_logistic_regression(X_test_manual, w_trained)\n",
        "\n",
        "# Calculate the errors manually\n",
        "train_error_manual = 1 - np.mean(y_train_pred_manual == y_train_manual)\n",
        "test_error_manual = 1 - np.mean(y_test_pred_manual == y_test_manual)\n",
        "\n",
        "train_error_manual, test_error_manual"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8MSiXrtyfOj",
        "outputId": "a454ff94-9eef-4873-8ae1-a1ae6647cf42"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4893617021276596, 0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We observed a notable enhancement in the accuracy of our logistic regression model upon the integration of L1 and L2 regularization. Prior to this adjustment, the model exhibited lower performance metrics on the same dataset. The regularization technique effectively mitigated the issue of overfitting, which is particularly beneficial in cases with numerous features or complex decision boundaries. By penalizing larger weights, the regularized model achieves a more generalized solution, leading to improved reliability and predictive accuracy. This improvement underscores the importance of regularization as a key technique in refining machine learning models, especially in scenarios prone to overfitting**"
      ],
      "metadata": {
        "id": "aASyldzBy0ao"
      }
    }
  ]
}